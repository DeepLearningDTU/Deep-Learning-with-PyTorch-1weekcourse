{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2cdb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee971c",
   "metadata": {},
   "source": [
    "# Transformers \n",
    "\n",
    "\n",
    "**Credits:** In this notebook you are going to use a Transformer model as described in the [\"Attention is all you need\"](https://arxiv.org/pdf/1706.03762.pdf) paper. This notebook closely follows the PyTorch tutorial on [language translation using Transformers](https://pytorch.org/tutorials/beginner/translation_transformer.html) but we will try to incorporate information from the [Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html), i.e. a nice description of the model presented in the paper by a professor from Harvard. If you are really interested in using transformers we really suggest you to go through the Annotated Transformer blog. Other interesting read for understand this model that are also partially used to build this notebook are: [This blog post](https://e2eml.school/transformers.html) and this [twitter thread](https://threadreaderapp.com/thread/1470406419786698761.html).\n",
    "\n",
    "For this notebook, we are providing a trained model, so you are not required to train anything, although we will give you the code to train a model. The main goal is to explain in-detail supported by a small code-snipet each \n",
    "\n",
    "**NOTE**: This notebook is far from being perfect since it was a \"last-minute addition\" and mostly because I am still learning about transformers and how to work with text while preparing this notebook. However, I wanted to give you a small introduction and example of a state-of-the-art model. I am still deciding which is the best way to present transformers (i.e. maybe using everything from scratch or instead using directly PyTorch implementations as we are doing) and which model should we use (seq2seq or just an autoregressive decoder). You feedback will be super useful and as soon this notebook is updated I will share it with you. At the end I am adding important reference that can help you in grasping this topic.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc73d02c",
   "metadata": {},
   "source": [
    "# Seq2Seq model architecture\n",
    "\n",
    "The model we are going to implement is a Sequence-to-Sequence model, which is usually used for language translations. Nowadays, state of the art for NLP, such as GPT-2 or BERT have a different structure, but some of the components are the same as the one we present here. For example GPT-2 is a decoder-only model, which is suitable for sentences generation, i.e. traditional language model, while the BERT is an encoder-model, which is useful for masked language model.\n",
    "\n",
    "The model we considered follow the architecture shown in the following figure and we are going to use it to translate from German to English.\n",
    "\n",
    "![Seq2Seq Transformer model](seq2seq_transformer.png)\n",
    "\n",
    "The model is defined by the following main components:\n",
    "- an input embedding\n",
    "- a positional encoding\n",
    "- an encoder (given by the left part of the architecture above) \n",
    "- a decoder (given by the right part of the architecture above)\n",
    "\n",
    "If we look at both the encoder and the decoder we can notice that there is an important components that appear in both and which is also one of the ideas that is really influencing every machine learning field which is the **attention mechanism**, and in this particular case the multi-head attention mechanism.\n",
    "\n",
    "While we are going to create the final model using directly the PyTorch library, we will try to analyze and familiarize with some of the components, just to show that something that sounds really defficult is instead pretty straight-forward to implement.\n",
    "\n",
    "## Embeddings\n",
    "\n",
    "As in other language models, we used embeddings to learn a new representation of size $d_{\\text{model}}$ for every input and autoregressively generated output token. In PyTorch we can easily implement this by using an embedding layer `nn.Embedding(vocab, d_model)`, where `vocab` is the dimension of the vocabulary, i.e. the number of unique words we are using and `d_model` is the emebdding dimension. In the paper, they suggest to  multiply those weights by $\\sqrt{d_{\\text{model}}}$. But what is actually doing an Embedding layer? Imagine you get a sentence with $n$ words and your vocabulary contains $N$ words. The embeddings can be represented as a matrix of size $N \\times d_{\\text{model}}$, and the embedding layer is just doing a matrix-multiplication between the one-hot-encoded version of each words in the sentence, that can be represented as a $n \\times N$ matrix, and the embedding matrix. This results in a $n \\times d_{\\text{model}}$ matrix. This is represented by the following image:\n",
    "\n",
    "![Embeddings computation](embeddings.png)\n",
    "\n",
    "This is can be done in Python by using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7f43bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding_layer(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb70d85",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "\n",
    "The position of a word in the sentence can make a difference in the type of meaning that word is assuming. In order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. While an RNN see the tokens sequntially and therefore can get an idea of the position of the different words in the sentence, a Transformer model is not processing the input sequentially, therefore this model needs a way to understand which part of the sentence it is processing.\n",
    "\n",
    "The positional encoding should satisy the following properties:\n",
    "- for each time-step, i.e. word position in a sentence, it should produce a unique vector;\n",
    "- the distance between any two time-steps should be consistent;\n",
    "- it should be deterministic.\n",
    "\n",
    "In the paper they proposed to use a positional encoding that produce a vector of size $d_{\\text{model}}$ as the embedding size, so we can add them together. The method proposed by the paper can be defined as:\n",
    "\n",
    "\\begin{align}\n",
    " PE(t,2i) &= \\sin{\\left(\\frac{t}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)} \\\\\n",
    " PE(t,2i+1) &= \\cos{\\left(\\frac{t}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)}\n",
    "\\end{align}\n",
    "\n",
    "where $t$ is the position of the world in the sentence and $i$ is the index in the embedding. To do this we have to consider $i=0,2,4,\\dots$. Indeed, then for each $i$ we compute both the functions above and thertefore $2i$ and $2i+1$ represents the column index of the positional encoding vector. In other words, for each even index $i$ we apply the first equation and for every odd index we apply the second one. Therefore, each dimension of the positional encoding corresponds to a sinusoid and the vector that we will obtain will have the following form:\n",
    "\n",
    "$$ PE = [ \\sin{(t)}, \\cos{(t)}, \\sin{\\left(\\frac{t}{10000^{\\frac{2}{d_{\\text{model}}}}}\\right)}, \\cos{\\left(\\frac{t}{10000^{\\frac{2}{d_{\\text{model}}}}}\\right)}, \\cdots]$$\n",
    "\n",
    "\n",
    "The class defining the Positional Encoding is given by this code snippet. We can also try to plot and look at the positional encoding we get, this is done in the second code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ec6a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "851f3722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's assume we havee a sequence of length 10 and embedding of length 128\n",
    "# how their positional encoding looks like\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# we define the positional encoding for a 128-dimensional vector and for this \n",
    "# example we do not use dropout\n",
    "pe = PositionalEncoding(128, 0)\n",
    "\n",
    "# note here we are assuming our input to be\n",
    "# batch_size x sequence_length x d_model\n",
    "# but in pytorch they are assuming \n",
    "# sequence_length x batch_size x d_model\n",
    "# we also assume zeros so we can see how the positional encoding looks like\n",
    "x_inputs = torch.zeros(1, 10, 128)\n",
    "y = pe(x_inputs)\n",
    "\n",
    "sns.heatmap(y.squeeze(0))\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Position in the sentence')\n",
    "plt.show()\n",
    "\n",
    "# in the figure below each row corresponds to the vector we are adding \n",
    "# to our embedding vector when the word is at that position in the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bece3f7",
   "metadata": {},
   "source": [
    "## Attention mechanism\n",
    "\n",
    "The concept of attention is not completely new. It was already introduced for RNN and LSTM. We will start by briefly explain the attention mechanism and then we will go through the Multi-Head Self Attention that was used in the paper.\n",
    "\n",
    "If we look at the difinition they used in the paper, we get that \"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\" \n",
    "\n",
    "The attention mechanism we are going to use is called \"Scaled Dot-Product Attention\" and it is shown in this Figure:\n",
    "\n",
    "![Scaled Dot-Product Attention](attention.png)\n",
    "\n",
    "By still using the description used in the paper, they say that \"The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.\"\n",
    "\n",
    "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\n",
    "\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax}{\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)}V$$\n",
    "\n",
    "One of the main challenges I had in the past days, was to understand what this $Q,K,V$ actually are and how we can compute them. I've found this [video](https://www.youtube.com/watch?v=8zAP2qWAsKg&t=2071s) super useful (Here you can access the [slides](https://storage.googleapis.com/deepmind-media/UCLxDeepMind_2020/L7%20-%20UCLxDeepMind%20DL2020.pdf)). I really invite you to watch it if you want to understand a bit better how this is compute. I will show one slide here and try to writwe my understanding (to be confirmed if they are correct though, I am sorry). I will try to report the main slides here and comment this.\n",
    "\n",
    "![Attention first step](slide_attention1.png)\n",
    "\n",
    "By looking at the slide above, we compute $Q,K,V$ by using the embedding we get after using the word embedding and the positional encoding. We compute this for each world (in the image above we are in the encoder and we can access both previous and future words, in the decoder part we have to access only the previous words). We will use our generated query $q$ we just computed for the dot product of our query with all the key vectors of the considered words in the considered sequence and then apply softmax. In our example this corresponds to creating $\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4$. \n",
    "\n",
    "![Attention second step](slide2.png)\n",
    "\n",
    "Then we will use these $\\lambda$s to weight the value of the considered word ('beeetle' in this case) and create the new representation for our word a weighted sum that takes into account the context where our word is inserted. \n",
    "\n",
    "![Attention last step](slide4.png)\n",
    "\n",
    "\n",
    "Now the Multi-Head Self Attention, which is represented in the next figure, is just a repeated computation of th eattention mechanism that we have just presented several times (in the figure below we are computing it $h$ times).\n",
    "\n",
    "![Multi-Head Attention](multi_head_attention.png)\n",
    "\n",
    "Here we show in a code snippet how you can implement from scratch this mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33dfa902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8b81b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c8e912",
   "metadata": {},
   "source": [
    "## Let's see how we can program this model in Pytorch\n",
    "\n",
    "Now that we have a better understanding (hopefully) of the different components, we can show how we can program in very few lines of PyTorch. We will use this model to translate from German to English. This tutorial is taken from the [PyTorch website](https://pytorch.org/tutorials/beginner/translation_transformer.html). \n",
    "\n",
    "**Note:** one of the most challenging things (that I still have to figure out, so I am already apologizing if I won't be able to answer your questions) is all the data preprocess and how the data is fed to the model for training. I am still figuring that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1433a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.8/site-packages (3.2.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.7.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.59.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2020.12.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.2.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.2.0/de_core_news_sm-3.2.0-py3-none-any.whl (19.1 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from de-core-news-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.20.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (20.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.59.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# packages we need for english and german vocabulary\n",
    "!python -m pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a34b9",
   "metadata": {},
   "source": [
    "### Data preprocessing \n",
    "\n",
    "In the following we are using `torchtext` library, which has utilities for creating datasets that can be easily iterated through for the purposes of creating a language translation model. In this example, we show how to use torchtext's inbuilt datasets, tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor. We will use Multi30k dataset, which contains different image captions from German to English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbb3b869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1.21M/1.21M [00:01<00:00, 861kB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "\n",
    "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download de_core_news_sm\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    " \n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator \n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ebae8b",
   "metadata": {},
   "source": [
    "## Seq2Seq Network using Transformer\n",
    "\n",
    "Below, we will create a Seq2Seq network that uses Transformer. The network consists of three parts. First part is the embedding layer. This layer converts tensor of input indices into corresponding tensor of input embeddings. These embedding are further augmented with positional encodings to provide position information of input tokens to the model. The second part is the actual Transformer model. Finally, the output of Transformer model is passed through linear layer that give un-normalized probabilities for each token in the target language.\n",
    "\n",
    "You can notice that the PositionalEncoding and the TokenEmbedding are the same as the one we have analyzed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c72a0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network \n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, \n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae37b1",
   "metadata": {},
   "source": [
    "During training, we need a subsequent word mask that will prevent model to look into the future words when making predictions. We will also need masks to hide source and target padding tokens. Below, let's define a function that will take care of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b6fc50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b2762",
   "metadata": {},
   "source": [
    "Let's now define the parameters of our model and instantiate the same. Below, we also \n",
    "define our loss function which is the cross-entropy loss and the optmizer used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c8fa9874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqTransformer(\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (generator): Linear(in_features=512, out_features=10838, bias=True)\n",
      "  (src_tok_emb): TokenEmbedding(\n",
      "    (embedding): Embedding(19215, 512)\n",
      "  )\n",
      "  (tgt_tok_emb): TokenEmbedding(\n",
      "    (embedding): Embedding(10838, 512)\n",
      "  )\n",
      "  (positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, \n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "\n",
    "print('Double checking the structure of our model')\n",
    "print(transformer)\n",
    "\n",
    "# initialization using xavier uniform\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a286e4c",
   "metadata": {},
   "source": [
    "## Collation\n",
    "\n",
    "We need to convert these string pairs into the batched tensors that can be processed by our Seq2Seq network defined previously. Below we define our collate function that convert batch of raw strings into batch tensors that can be fed directly into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e427385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "# BOS = Beginning of sentence\n",
    "# EOS = End of sentence\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c614d9a",
   "metadata": {},
   "source": [
    "Let's define training and evaluation loop that will be called for each \n",
    "epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ef30eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# This function define all the operation needed to train our model for one epoch\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    \n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "#function to evaluate the model on the validation set\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        \n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b26b222",
   "metadata": {},
   "source": [
    "### Loading pre-trained model\n",
    "Training this model will takes ~20mins. Therefore we are providing you some already trained weigths. You can find them here: https://drive.google.com/drive/folders/1gyBwlT1G2XnnoUyCDK4ndZZq9AFh_m4C. You should download them and them drag them into colab. How can you do that? On the left side of colab session there is a folder icon. Press that, you should see one folder called \"sample data\". Then you can drag the weights in the same space (be careful: you should **not** drag them into the \"sample data\" folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67c7415e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "already_trained = True\n",
    "\n",
    "if already_trained:\n",
    "    if DEVICE.type == 'cpu':\n",
    "        transformer.load_state_dict(torch.load('transformer_weights.pt', map_location=torch.device('cpu')))\n",
    "        print('Weights loaded')\n",
    "    else:\n",
    "        transformer.load_state_dict(torch.load('transformer_weights.pt'))\n",
    "        print('Weights loaded')\n",
    "else:\n",
    "    best_val_error = 100\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        start_time = timer()\n",
    "        train_loss = train_epoch(transformer, optimizer)\n",
    "        end_time = timer()\n",
    "        val_loss = evaluate(transformer)\n",
    "        print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "        if val_loss < best_val_error:\n",
    "            print(f'Better val error in epoch {epoch}')\n",
    "            torch.save(transformer.state_dict(), 'transformer_weigths.pt')\n",
    "\n",
    "\n",
    "# function to generate output sequence using greedy algorithm \n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8a502a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A group of people stand in front of an office store . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e69aaa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The car is red . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, \"Das Auto ist rot .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "170deb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A city in the country . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, \"Eine Stadt auf dem Land .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c6b3e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A man is in a dog . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, \"Ein Mann ist mit einem Hund .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84eb0612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A lot of flowers is standing on the table . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, \"Eine Vase mit Blumen steht auf dem Tisch .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ad224d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A car is driving on the highway . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, \"Ein Auto fährt auf der Autobahn .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aced71ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The players are cheering for the goal . \n"
     ]
    }
   ],
   "source": [
    "print(translate(transformer, \"Die Spieler jubeln nach dem Tor .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b36c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7e8e669",
   "metadata": {},
   "source": [
    "### Additional reference\n",
    "\n",
    "Here I leave some references that can be used for understanding a bit better this complicated model (most of them I already linked while explaining some stuff):\n",
    "- [Video from the main author of the paper](https://www.youtube.com/watch?v=rBCqOTEfxvg&t=894s)\n",
    "- [Video lecture from Deepmind](https://www.youtube.com/watch?v=8zAP2qWAsKg&t=2073s)\n",
    "- [Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "- [Blog Post on the Positional Encoder](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n",
    "- [Transformer from Scratch](https://e2eml.school/transformers.html#positional_encoding)\n",
    "- [Another blog post explaining transformer](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45c4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ed0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
