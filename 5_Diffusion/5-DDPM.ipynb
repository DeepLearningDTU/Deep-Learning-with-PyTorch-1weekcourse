{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Models\n",
    "Exercise by [Jes Frellsen](https://frellsen.org) (Technical University of Denmark), 21 March 2024 (version 1.0).\n",
    "\n",
    "\n",
    "The main task in this week's programming exercise is to implement tDenoising Diffusion Probabilistic Model (DDPM) by [Ho et al., 2020](https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html). We will start with the two simple toy datasets called TwoGaussian and Chequerboard (illustrated below), and then we will move on to learning DDPMs on MNIST.\n",
    "\n",
    "We have provided you with two files:\n",
    "* `Unet.py` contains the code for a U-Net predicting $\\epsilon$ of reverse process on MNIST. The architecture and the implementation of the U-Net is adapted from an implementation by\n",
    "[Muhammad Firmansyah Kasim](https://github.com/mfkasim1/score-based-tutorial/blob/main/03-SGM-with-SDE-MNIST.ipynb).\n",
    "* `ToyData.py` contains the code for generating data from the two toy models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the files using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -O https://raw.githubusercontent.com/DeepLearningDTU/Deep-Learning-with-PyTorch-1weekcourse/master/5_Diffusion/ToyData.py\n",
    "! curl -O https://raw.githubusercontent.com/DeepLearningDTU/Deep-Learning-with-PyTorch-1weekcourse/master/5_Diffusion/Unet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy data\n",
    "First we visualize the probability densities for the toy datasets.\n",
    "\n",
    "When we create an object of the `Chequerboard` or `TwoGaussian`, we can call the forward method which returns a `Distribution` object from `torch.distributions`. The `Distribution` class implements a method for calculating the log probability (`log_prob(...)`), which we will use to make the plots below, and method for sampling from the distribuion (`sample(...)`), which we will later use for creating our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ToyData\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Make a density plot of the Checkerboard distribution\n",
    "toy = ToyData.Chequerboard()\n",
    "coordinates = [[[x,y] for x in np.linspace(*toy.xlim, 1000)] for y in np.linspace(*toy.ylim, 1000)]\n",
    "prob = torch.exp(toy().log_prob(torch.tensor(coordinates)))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "im = ax1.imshow(prob, extent=[toy.xlim[0], toy.xlim[1], toy.ylim[0], toy.ylim[1]], origin='lower', cmap='YlOrRd')\n",
    "ax1.set_xlim(toy.xlim)\n",
    "ax1.set_ylim(toy.ylim)\n",
    "ax1.set_aspect('equal')\n",
    "cbar1 = fig.colorbar(im, ax=ax1)\n",
    "ax1.set_title('Checkerboard distribution')\n",
    "cbar1.set_label('Probability density')\n",
    "\n",
    "# Make a density plot of the Gaussian distribution\n",
    "toy = ToyData.TwoGaussians()\n",
    "coordinates = [[[x,y] for x in np.linspace(*toy.xlim, 1000)] for y in np.linspace(*toy.ylim, 1000)]\n",
    "prob = torch.exp(toy().log_prob(torch.tensor(coordinates)))\n",
    "\n",
    "im = ax2.imshow(prob, extent=[toy.xlim[0], toy.xlim[1], toy.ylim[0], toy.ylim[1]], origin='lower', cmap='YlOrRd')\n",
    "ax2.set_xlim(toy.xlim)\n",
    "ax2.set_ylim(toy.ylim)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.set_title('Two Gaussians distribution')\n",
    "cbar2 = fig.colorbar(im, ax=ax2)\n",
    "cbar2.set_label('Probability density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the DDPM\n",
    "**DDPM implementation:** Below we provide an implementation of the DDPM. The code is missing the implementation of the loss and sample function. Your task will be to complete them (see Exercise 1 below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, network, beta_1=1e-4, beta_T=2e-2, T=100):\n",
    "        \"\"\"\n",
    "        Initialize a DDPM model.\n",
    "\n",
    "        Parameters:\n",
    "        network: [nn.Module]\n",
    "            The network to use for the diffusion process.\n",
    "        beta_1: [float]\n",
    "            The noise at the first step of the diffusion process.\n",
    "        beta_T: [float]\n",
    "            The noise at the last step of the diffusion process.\n",
    "        T: [int]\n",
    "            The number of steps in the diffusion process.\n",
    "        \"\"\"\n",
    "        super(DDPM, self).__init__()\n",
    "        self.network = network\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_T = beta_T\n",
    "        self.T = T\n",
    "\n",
    "        self.beta = nn.Parameter(torch.linspace(beta_1, beta_T, T), requires_grad=False)\n",
    "        self.alpha = nn.Parameter(1 - self.beta, requires_grad=False)\n",
    "        self.alpha_cumprod = nn.Parameter(self.alpha.cumprod(dim=0), requires_grad=False)\n",
    "    \n",
    "    def negative_elbo(self, x):\n",
    "        \"\"\"\n",
    "        Evaluate the DDPM negative ELBO on a batch of data.\n",
    "\n",
    "        Parameters:\n",
    "        x: [torch.Tensor]\n",
    "            A batch of data (x) of dimension `(batch_size, *)`.\n",
    "        Returns:\n",
    "        [torch.Tensor]\n",
    "            The negative ELBO of the batch of dimension `(batch_size,)`.\n",
    "        \"\"\"\n",
    "\n",
    "        ### Implement Algorithm 1 here ###\n",
    "        neg_elbo = None\n",
    "\n",
    "        return neg_elbo\n",
    "\n",
    "    def sample(self, shape):\n",
    "        \"\"\"\n",
    "        Sample from the model.\n",
    "\n",
    "        Parameters:\n",
    "        shape: [tuple]\n",
    "            The shape of the samples to generate.\n",
    "        Returns:\n",
    "        [torch.Tensor]\n",
    "            The generated samples.\n",
    "        \"\"\"\n",
    "        # Sample x_t for t=T (i.e., Gaussian noise)\n",
    "        x_t = torch.randn(shape).to(self.alpha.device)\n",
    "\n",
    "        # Sample x_t given x_{t+1} until x_0 is sampled\n",
    "        for t in range(self.T-1, -1, -1):\n",
    "            ### Implement the remaining of Algorithm 2 here ###\n",
    "            pass\n",
    "\n",
    "        return x_t\n",
    "\n",
    "    def loss(self, x):\n",
    "        \"\"\"\n",
    "        Evaluate the DDPM loss on a batch of data.\n",
    "\n",
    "        Parameters:\n",
    "        x: [torch.Tensor]\n",
    "            A batch of data (x) of dimension `(batch_size, *)`.\n",
    "        Returns:\n",
    "        [torch.Tensor]\n",
    "            The loss for the batch.\n",
    "        \"\"\"\n",
    "        return self.negative_elbo(x).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training loop**: We have also implemented a generic training loop for learning the DDPM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(model, optimizer, data_loader, epochs, device):\n",
    "    \"\"\"\n",
    "    Train a Flow model.\n",
    "\n",
    "    Parameters:\n",
    "    model: [Flow]\n",
    "       The model to train.\n",
    "    optimizer: [torch.optim.Optimizer]\n",
    "         The optimizer to use for training.\n",
    "    data_loader: [torch.utils.data.DataLoader]\n",
    "            The data loader to use for training.\n",
    "    epochs: [int]\n",
    "        Number of epochs to train for.\n",
    "    device: [torch.device]\n",
    "        The device to use for training.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    total_steps = len(data_loader)*epochs\n",
    "    progress_bar = tqdm(range(total_steps), desc=\"Training\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        data_iter = iter(data_loader)\n",
    "        for x in data_iter:\n",
    "            if isinstance(x, (list, tuple)):\n",
    "                x = x[0]\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(loss=f\"â €{loss.item():12.4f}\", epoch=f\"{epoch+1}/{epochs}\")\n",
    "            progress_bar.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Network:** We will use a fully connect network for the mean network on the toy data. The `forward` function takes as input the data `x` and the time step `t` and concatenates them before sending them though the fully connected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FcNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden):\n",
    "        \"\"\"\n",
    "        Initialize a fully connected network for the DDPM, where the forward function also take time as an argument.\n",
    "        \n",
    "        parameters:\n",
    "        input_dim: [int]\n",
    "            The dimension of the input data.\n",
    "        num_hidden: [int]\n",
    "            The number of hidden units in the network.\n",
    "        \"\"\"\n",
    "        super(FcNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(nn.Linear(input_dim+1, num_hidden), nn.ReLU(), \n",
    "                                     nn.Linear(num_hidden, num_hidden), nn.ReLU(), \n",
    "                                     nn.Linear(num_hidden, input_dim))\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\"\n",
    "        Forward function for the network.\n",
    "        \n",
    "        parameters:\n",
    "        x: [torch.Tensor]\n",
    "            The input data of dimension `(batch_size, input_dim)`\n",
    "        t: [torch.Tensor]\n",
    "            The time steps to use for the forward pass of dimension `(batch_size, 1)`\n",
    "        \"\"\"\n",
    "        x_t_cat = torch.cat([x, t], dim=1)\n",
    "        return self.network(x_t_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training data**: Next, we generate some training data from the TwoGaussians datasets and create a `data_loader`. We generate a dataset with 10M data points and use a large batch size of 10,000. We can do so, since it is only a two-dimensional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "import ToyData\n",
    "\n",
    "n_data = 10000000\n",
    "batch_size = 10000\n",
    "\n",
    "toy = ToyData.TwoGaussians()\n",
    "transform = lambda x: (x-0.5)*2.0\n",
    "train_loader = torch.utils.data.DataLoader(transform(toy().sample((n_data,))), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the model and run the training loop**: Finally we initializes the model and run the training loop. Remember that this will not work before you have completed the assignment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimension of the dataset\n",
    "D = next(iter(train_loader)).shape[1]\n",
    "\n",
    "# Define the network\n",
    "num_hidden = 64\n",
    "network = FcNetwork(D, num_hidden)\n",
    "\n",
    "# Set the number of steps in the diffusion process\n",
    "T = 1000\n",
    "\n",
    "# Define model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DDPM(network, T=T).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Train model\n",
    "epochs = 1\n",
    "train(model, optimizer, train_loader, epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling**: The following code samples from a trained model and plots the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate samples\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    samples = (model.sample((10000,D))).cpu() \n",
    "\n",
    "    # Transform the samples back to the original space\n",
    "    samples = samples /2 + 0.5\n",
    "\n",
    "# Plot the density of the toy data and the model samples\n",
    "coordinates = [[[x,y] for x in np.linspace(*toy.xlim, 1000)] for y in np.linspace(*toy.ylim, 1000)]\n",
    "prob = torch.exp(toy().log_prob(torch.tensor(coordinates)))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "im = ax.imshow(prob, extent=[toy.xlim[0], toy.xlim[1], toy.ylim[0], toy.ylim[1]], origin='lower', cmap='YlOrRd')\n",
    "ax.scatter(samples[:, 0], samples[:, 1], s=1, c='black', alpha=0.5)\n",
    "ax.set_xlim(toy.xlim)\n",
    "ax.set_ylim(toy.ylim)\n",
    "ax.set_aspect('equal')\n",
    "fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Complete the DDPM implementation above, by implementing the following parts:\n",
    "* `DDPM.negative_elbo(...) should return the negative ELBO of equation (14) from the DDPMs paper ([Ho et al., 2020](https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html)) by implementing Algorithm 1 in the paper.\n",
    "* `DDPM.sample(shape)` should implement Algorithm 2 in the DDPMs paper ([Ho et al., 2020](https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html)). For the covariance matrix of the reverse process use $\\sigma^2_t\\mathbf{I}$ with $\\sigma^2_t=\\beta_t$. For example, to sample 5000 samples for the 2D toy examples, the method would be called with the argument `(5000, 2)`.\n",
    "\n",
    "A simple, fully connected network is implemented in the class `FcNetwork`. Note that the method `FcNetwork.forward(x, t)|` takes as input a batch of data `x` of dimension `(batch_size, input_dim)|` and the time step for each data point in the batch of dimension `(batch_size, 1)`. The method concatenates the data and the time step before inputting it to the network, and it is a good idea to normalize the time step to $[0,1]$.\n",
    "\n",
    "Test the implementation on both the TwoGaussians and Chequerboard datasets and answer the following questions:\n",
    "* Can you improve the fit to the Chequerboard dataset by modifying the network architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Use the DPPM implementation from Exercise 1 to learn a DDPM on MNIST. You *do not* need to implement a discrete likelihood function for the DDPM as suggested in section 3.3 by DDPMs paper ([Ho et al., 2020](https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html)). Instead, we will [dequantize](https://jmtomczak.github.io/blog/3/3_flows.html#Coupling-layers,-permutation-layers-and-dequantization) the pixel values (i.e., add noise to them) and transform them to $[-1, 1]$, which can be done with the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x + torch.rand(x.shape)/255),\n",
    "            transforms.Lambda(lambda x: (x-0.5)*2.0),\n",
    "            transforms.Lambda(lambda x: x.flatten())]\n",
    "            )\n",
    "\n",
    "train_data = datasets.MNIST('data/', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to transform the pixel values back to $[0,1]$ before displaying samples.\n",
    "\n",
    "You should both test a fully connected architecture and the provided U-Net architecture (in `Unet.py`). Please answer the following questions:\n",
    "* Can you learn a DDPM on MNIST using a fully connected architecture?\n",
    "* How do the samples from the DDPM qualitatively compare to the VAE from the VAE exercise?\n",
    "\n",
    "*Hint:* Remember to change the batch size to, e.g., 64, and you will need to train the model for around 50-100 epochs to get a good model, which takes around 15 minutes on a GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint:* If you have a mini batch of samples of images, you can display them in the notebook using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "image_tensor = torch.rand(64, 784)\n",
    "image_pil = to_pil_image(make_grid(image_tensor.view(-1, 1, 28, 28)))\n",
    "display(image_pil)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_02456",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
